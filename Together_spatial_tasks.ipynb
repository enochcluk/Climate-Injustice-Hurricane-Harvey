{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enochcluk/Climate-Injustice-Hurricane-Harvey/blob/master/Together_spatial_tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install together\n",
        "from together import Together\n"
      ],
      "metadata": {
        "id": "1OSAogmRuqgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dict = {\n",
        "    \"Google Gemma 7B IT\": \"google/gemma-7b-it\",\n",
        "    \"Google Gemma 2B IT\": \"google/gemma-2b-it\",\n",
        "    \"Meta LLaMA-3 Chat (8B)\": \"meta-llama/Llama-3-8b-chat-hf\",\n",
        "    \"Meta LLaMA-3 Chat (70B)\": \"meta-llama/Llama-3-70b-chat-hf\",\n",
        "    \"Mixtral-8x22B Instruct (141B)\": \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
        "}"
      ],
      "metadata": {
        "id": "fF60kmPxihDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1 = \"You have been given a circular path consisting of 12 connected dots. At the start, you are positioned on the dot that is located at the top of the path, where you find a crash helmet. Moving in a clockwise direction from the crash helmet, the elements on the path are a trimaran, a sleeping bag, a dhole, a sulphur butterfly, a stove, a rotisserie, a stupa, a bagel, a gibbon, a fountain, and a Geoffroy's spider monkey. Starting from the rotisserie, you move around the ring by 11 steps in a counter-clockwise direction, and you move around the ring by 4 steps in a clockwise direction, and you move around the ring by 11 steps in a counter-clockwise direction, and you move around the ring by 7 steps in a counter-clockwise direction, and you move around the ring by 2 steps in a clockwise direction. What will you find?\""
      ],
      "metadata": {
        "id": "yjp63z1AenNe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import re\n",
        "import glob\n",
        "import requests\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "import numpy as np\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "datapath = \"/content/drive/MyDrive/CS 159/data/raw/SpatialEvalLLM/\""
      ],
      "metadata": {
        "id": "oN94QszBcCve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d29d3f15-742e-4e8d-f43d-6ef5c09a3a2e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"6c3bf809d032c5580344edbb1123cfbfd202520aeedfe946f70039c03f2a8d82\"\n",
        "\n",
        "class llama8b:\n",
        "    def __init__(self, api_key):\n",
        "        self.client = Together(api_key=api_key)\n",
        "\n",
        "    def predict(self, prompt):\n",
        "        response = self.client.chat.completions.create(\n",
        "            model='meta-llama/Llama-3-8b-chat-hf',\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "model = llama8b(api_key)"
      ],
      "metadata": {
        "id": "0LoL9t0HaEQV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class gemma2b: #how to make a language/code model client\n",
        "#     def __init__(self, api_key):\n",
        "#         self.client = Together(api_key=api_key)\n",
        "\n",
        "#     def predict(self, prompt):\n",
        "#         response = self.client.completions.create(\n",
        "#             model='google/gemma-2b',\n",
        "#             prompt=prompt,\n",
        "#             max_tokens=1500\n",
        "#         )\n",
        "#         return response.choices[0].text"
      ],
      "metadata": {
        "id": "_xQ9KzT1ePLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_answer(response, use_code):\n",
        "    # extract answer based on whether code is used\n",
        "    if use_code:\n",
        "        try:\n",
        "            code_match = re.search(r\"###code###(.*?)###endcode###\", response, re.DOTALL)\n",
        "            code_to_execute = code_match.group(1).strip()\n",
        "            local_env = {}\n",
        "            exec(code_to_execute, {}, local_env)\n",
        "            return local_env.get(\"result\", \"No result produced\")  # Assume the code defines 'result'\n",
        "        except Exception as e:\n",
        "            return str(e)\n",
        "    else:\n",
        "        ans = re.search(r\"\\*\\*\\*(.+?)\\*\\*\\*\", response)\n",
        "        return ans.group(1).strip() if ans else \"No answer found\"\n",
        "\n",
        "def evaluate_file(file, model, leading_prompt, trailing_prompt, use_code, store_intermediate=True):\n",
        "    data_df = pd.read_json(file, lines=True)\n",
        "    if store_intermediate:\n",
        "        columns = [\"Question\", \"Expected Answer\", \"Model Response\", \"Extracted Answer\", \"Is Correct\"]\n",
        "    else:\n",
        "        columns = [\"Is Correct\"]\n",
        "\n",
        "    results_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "    for index, row in data_df.iterrows():\n",
        "        question = leading_prompt + row[\"question\"] + trailing_prompt\n",
        "        expected_answer = row[\"answer\"].lower()  # ensure the answer is in lower case\n",
        "        response = model.predict(question)\n",
        "        extracted_answer = extract_answer(response, use_code)\n",
        "        is_correct = extracted_answer.lower() == expected_answer\n",
        "\n",
        "        if store_intermediate:\n",
        "            results_df.loc[index] = [row[\"question\"], expected_answer, response, extracted_answer, is_correct]\n",
        "        else:\n",
        "            results_df.loc[index] = [is_correct]\n",
        "\n",
        "    return results_df\n",
        "\n",
        "def evaluate_across_files(model, leading_prompt, data_path, which_files=\"all\", use_code=False):\n",
        "    map_global_files = glob.glob(data_path + \"map_global/*\") #describe the map top down\n",
        "    map_local_files = glob.glob(data_path + \"map_local/*\") #don't\n",
        "\n",
        "    if which_files == \"all\":\n",
        "        these_files = map_global_files + map_local_files\n",
        "    elif which_files == \"local\":\n",
        "        these_files = map_local_files\n",
        "    elif which_files == \"global\":\n",
        "        these_files = map_global_files\n",
        "    else:\n",
        "        raise ValueError(f\"which_files={which_files} not supported\")\n",
        "\n",
        "    # set appropriate trailing prompt\n",
        "    if use_code:\n",
        "        trailing_prompt = \"Output a python script to find the answer using ###code### delimiters ###endcode###. No print statements. Set the variable 'result' to the answer, in lower case.\"\n",
        "    else:\n",
        "        trailing_prompt = \"Place the answer, in lower case, no 'the' or 'a', in stars ***like this***.\"\n",
        "\n",
        "    # evaluate each file and store the results\n",
        "    scores_df = pd.DataFrame(columns=[\"Filename\", \"Correct\", \"Total\", \"Accuracy\"])\n",
        "\n",
        "    for file in tqdm(these_files, desc=\"Evaluating files\"):\n",
        "        result = evaluate_file(file, model, leading_prompt, trailing_prompt, use_code, False)\n",
        "        num_correct = result[\"Is Correct\"].sum()\n",
        "        num_total = len(result)\n",
        "        scores_df.loc[len(scores_df)] = [file, num_correct, num_total, num_correct / num_total]\n",
        "\n",
        "    return scores_df\n"
      ],
      "metadata": {
        "id": "bQ03myaElSxC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "leading_prompt = \"Answer the following question. \"\n",
        "use_code = True\n",
        "start = time.time()\n",
        "if use_code:\n",
        "        trailing_prompt = \"Output a python script to find the answer using ###code### delimiters ###endcode###. No print statements. Set the variable 'result' to the answer, in lower case.\"\n",
        "else:\n",
        "    trailing_prompt = \"Place the answer, in lower case, no 'the' or 'a', in stars ***like this***.\"\n",
        "\n",
        "use_code = True\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/CS 159/data/raw/SpatialEvalLLM/map_global/type-ring_size-12_steps-8_seed-12_n-100.jsonl\"\n",
        "\n",
        "results_df = evaluate_file(file_path, model, leading_prompt, trailing_prompt, use_code, store_intermediate=True)\n",
        "\n",
        "\n",
        "print(results_df)\n",
        "print(time.time() - start)"
      ],
      "metadata": {
        "id": "M-eZjdYDwNzn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "d903ff8f-6e34-4b0e-9fb3-6dd79c66088f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'lower'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-e81ab225e618>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/CS 159/data/raw/SpatialEvalLLM/map_global/type-ring_size-12_steps-8_seed-12_n-100.jsonl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleading_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrailing_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore_intermediate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-cf3e86e4fa25>\u001b[0m in \u001b[0;36mevaluate_file\u001b[0;34m(file, model, leading_prompt, trailing_prompt, use_code, store_intermediate)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mextracted_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mis_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextracted_answer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpected_answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstore_intermediate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_correct = results_df[\"Is Correct\"].sum()\n",
        "num_total = len(results_df)"
      ],
      "metadata": {
        "id": "TVLkr3mKnl0G"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_correct/num_total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aR58mHHGnp82",
        "outputId": "1dfc3cc7-26e1-477b-d586-a4fae6806c01"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8uLAg2SAwhJT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}